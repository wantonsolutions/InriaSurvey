\section{Deadlocks in Datacenter Networks: Why Do They Form, and How to Avoid Them \\ \small{Shuihai Hu, Yibo Zhu, Peng Cheng, Chuanxiong Guo, Kun Tan, Jitendra Padhye, Kai Chen}}

\subsection{notes}

From the intro and problem definition this paper is of little interest to me. The authors state that there should be more research into the feild of deadlocks in networks with lossless messages.

Their reson is that RoCE requires lossless communication channels, these
channels use pauses to mittigate traffic and prevent overflow. If there is a
cycle a deadlock can occur. They cite ~\cite{rdma-commodity-ethernet-scale},
from andys class, where deadlocks occured because of some gap in ARP updates.

While this is true you really have to buy into RDMA, and believe that lossless channels are a real thing.

The key takeaway here is the formula $r > r_d = \frac{nB}{TTL}$ which
essentially sates that if the incomming amount of data is larger than what is
being taken out the system will deadlock. It ties into the idea that rd is
mittigated by the $TTL$ of any given packet.

\subsection{observations}

\subsection{questions}
